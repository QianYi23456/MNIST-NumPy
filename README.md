# 基于NumPy实现的MNIST
## 项目描述
这是一个使用 NumPy 实现的 MNIST 手写数字识别项目，旨在从零开始构建一个简单的神经网络，无需依赖深度学习框架。
## 功能特性
- 使用 NumPy 实现神经网络的前向传播和反向传播。
- 支持 MNIST 数据集的加载和预处理。
- 提供简单的训练和测试接口。
## 安装步骤
克隆仓库：
   ```bash
   git clone https://github.com/QianYi23456/MNIST-NumPy.git
   ```
依赖环境：
   NumPy+Sklearn
## 运行结果
### 学习率的影响
- 循环轮数：20，隐藏层大小：128，分组大小：128，学习率：0.01，损失值Loss从0.6966逐步下降到0.4047，准确率为93.47%
- 循环轮数：20，隐藏层大小：128，分组大小：128，学习率：0.1，损失值Loss从0.4283逐步下降到0.0776，准确率为97.07%
- 循环轮数：20，隐藏层大小：128，分组大小：128，学习率：0.2，损失值Loss从0.3238逐步下降到0.0418，准确率为97.39%
- 循环轮数：20，隐藏层大小：128，分组大小：128，学习率：0.3，损失值Loss从0.2343逐步下降到0.0198，准确率为97.57%
- 循环轮数：20，隐藏层大小：128，分组大小：128，学习率：0.5，损失值Loss从0.1609逐步下降到0.0112，准确率为97.77%
- 循环轮数：20，隐藏层大小：128，分组大小：128，学习率：0.7，损失值Loss从0.123逐步下降到0.0042，准确率为97.81%
- 循环轮数：20，隐藏层大小：128，分组大小：128，学习率：0.9，损失值Loss从0.1713逐步下降到0.0089，准确率为97.7%
  当学习率从0.01逐步增大到0.9时，再20轮循环后，损失值最终控制在0.1以下，准确率在达到97%以后增长缓慢，并在0.7-0.9之间，准确率有所下降。
### 隐藏层的影响
- 循环轮数：20，隐藏层大小：64，分组大小：128，学习率：0.2，损失值Loss从0.329逐步下降到0.0282，准确率为97.11%
- 循环轮数：20，隐藏层大小：128，分组大小：128，学习率：0.2，损失值Loss从0.3238逐步下降到0.0418，准确率为97.39%
- 循环轮数：20，隐藏层大小：256，分组大小：128，学习率：0.2，损失值Loss从0.2071逐步下降到0.0143，准确率为97.74%
- 循环轮数：20，隐藏层大小：512，分组大小：128，学习率：0.2，损失值Loss从0.1915逐步下降到0.0209，准确率为97.76%
  从数据中可以看出，隐藏层的数据变化对准确率的影响较为显著。
### 循环轮数的影响
- 循环轮数：10，隐藏层大小：128，分组大小：128，学习率：0.2，损失值Loss从0.3238逐步下降到0.0523，准确率为97.03%
- 循环轮数：30，隐藏层大小：128，分组大小：128，学习率：0.2，损失值Loss从0.3238逐步下降到0.0131，准确率为97.62%
- 循环轮数：40，隐藏层大小：128，分组大小：128，学习率：0.2，损失值Loss从0.3238逐步下降到0.0072，准确率为97.61%
  循环轮数在30-40之间，略微有所下降。
## 结论
从该实验结果可看出，调整适当的参数，能够达到识别的准确率在97%-98%之间。
