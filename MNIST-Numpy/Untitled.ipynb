{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8899822-ea34-40dc-a3a6-8e223aa362cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3238\n",
      "Epoch 2, Loss: 0.2219\n",
      "Epoch 3, Loss: 0.2007\n",
      "Epoch 4, Loss: 0.0897\n",
      "Epoch 5, Loss: 0.1276\n",
      "Epoch 6, Loss: 0.0807\n",
      "Epoch 7, Loss: 0.0540\n",
      "Epoch 8, Loss: 0.0392\n",
      "Epoch 9, Loss: 0.0189\n",
      "Epoch 10, Loss: 0.0523\n",
      "Epoch 11, Loss: 0.0148\n",
      "Epoch 12, Loss: 0.0100\n",
      "Epoch 13, Loss: 0.0606\n",
      "Epoch 14, Loss: 0.0419\n",
      "Epoch 15, Loss: 0.0110\n",
      "Epoch 16, Loss: 0.1207\n",
      "Epoch 17, Loss: 0.1000\n",
      "Epoch 18, Loss: 0.0052\n",
      "Epoch 19, Loss: 0.0163\n",
      "Epoch 20, Loss: 0.0418\n",
      "Test Accuracy: 0.9739\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载MNIST数据集\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')  #从OPenML加载MNIST数据集\n",
    "\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(np.uint8)  #分别获取特征数据X，和标签数据y，并将标签数据y转换为整数类型\n",
    "X = X / 255.0  # 将像素值归一化到[0,1]，方便模型训练\n",
    "\n",
    "# 转换为One-Hot标签\n",
    "def one_hot(y, num_classes):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_one_hot = one_hot(y, 10)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)  #训练集的数据占比是80%，测试集的数据占比是20%\n",
    "\n",
    "\n",
    "#初始化参数\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)   #指定一个固定的seed值\n",
    "    W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)  # He初始化，He初始化是一种用于神经网络的权重初始化方法特别适用于ReLU\n",
    "    b1 = np.zeros(hidden_size)       #偏置向量b1全为零\n",
    "    W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)  #2.0 / input_size 用于缩放因子的分母部分，该初始化的目标是确保每一层输出的方差保持一致，避免梯度消失或爆炸问题\n",
    "    b2 = np.zeros(output_size)        #偏置向量b2全为零\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "#ReLu激活函数，返回(0,Z)的最大值\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "#Softmax函数，将输入转换为概率分布\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # 防止数值溢出，因为Z中某些值可能很大，直接计算以e为底的指数，其结果可能会非常大，甚至可能超出计算机的计算范围\n",
    "    return exp_Z / exp_Z.sum(axis=1, keepdims=True)  #这里防溢出处理是Z中每行的数据都减去该行的最大值，如此该行的最大值变为0，其他元素变为负数或接近0，这样便将指数运算的结果控制在一个合理范围\n",
    "\n",
    "#前向传播\n",
    "def forward(X, parameters):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    Z1 = np.dot(X, W1) + b1   #点乘\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "\n",
    "#计算损失\n",
    "def compute_loss(Y, Y_hat):  #Y是真实值，Y_hat是预测值\n",
    "    m = Y.shape[0]\n",
    "    loss = -np.sum(Y * np.log(Y_hat + 1e-8)) / m  # 计算交叉熵损失，加入1e-8以防止 log(0) 导致数值变成负无穷溢出\n",
    "    return loss\n",
    "\n",
    "#反向传播\n",
    "def backward(X, Y, parameters, forward_cache):\n",
    "    W1, W2 = parameters[\"W1\"], parameters[\"W2\"]    #W1和W2是权重系数\n",
    "    A1, A2 = forward_cache[\"A1\"], forward_cache[\"A2\"]  #A2是输出层的结果，A1是中间隐藏层经过ReLU函数的结果\n",
    "    Z1 = forward_cache[\"Z1\"]    #从前向传播的结果中获取隐藏层未ReLU函数前的结果，以便进行链式求偏导\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # 输出层梯度\n",
    "    dZ2 = (A2 - Y) / m  # Softmax交叉熵的梯度简化为(A2 - Y)，并除以m以取平均\n",
    "    dW2 = np.dot(A1.T, dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0)\n",
    "\n",
    "    # 隐藏层梯度\n",
    "    dZ1 = np.dot(dZ2, W2.T) * (Z1 > 0)  # 乘以ReLU的导数：当 Z1 > 0 时导数为1，否则为0\n",
    "    dW1 = np.dot(X.T, dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0)\n",
    "\n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "#更新参数，使用梯度下降法\n",
    "def update_parameters(parameters, grads, learning_rate=0.01):\n",
    "    parameters[\"W1\"] -= learning_rate * grads[\"dW1\"]    #更新w1\n",
    "    parameters[\"b1\"] -= learning_rate * grads[\"db1\"]    \n",
    "    parameters[\"W2\"] -= learning_rate * grads[\"dW2\"] \n",
    "    parameters[\"b2\"] -= learning_rate * grads[\"db2\"] \n",
    "    return parameters\n",
    "\n",
    "\n",
    "#训练模型\n",
    "def train(X_train, y_train, epochs=100, batch_size=128, lr=0.01):\n",
    "\n",
    "   #X_train是pandas里的DataFrame结构，这里要将其转换为NumPy结构，否则下面的X_shuffled = X_train[permutation]会报错\n",
    "    X_train = X_train.values\n",
    "    \n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_size = 128\n",
    "    output_size = 10\n",
    "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        permutation = np.random.permutation(X_train.shape[0])\n",
    "        X_shuffled = X_train[permutation]     #permutation是一个整数数组，表示打乱后的行索引，如果没有上面那行代码， X_train[permutation]试图用整数索引访问DataFrame的列，但DataFrame的列名是字符串\n",
    "        y_shuffled = y_train[permutation]    #这里不会报错，因为y_train是Numpy结构\n",
    "        \n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # 前向传播\n",
    "            cache = forward(X_batch, parameters)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = compute_loss(y_batch, cache[\"A2\"])\n",
    "            \n",
    "            # 反向传播\n",
    "            grads = backward(X_batch, y_batch, parameters, cache)\n",
    "            \n",
    "            # 参数更新\n",
    "            parameters = update_parameters(parameters, grads, lr)\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "#计算准确率\n",
    "def accuracy(y_true, y_pred):                                            #np.argmax(y_true, axis=1)表示y_true沿axis=1(即每一行)获取该行最大值的索引\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))  #当y_true表示的真实值与y_pred表示的预测值，在两者对应的每一行的最大值索引相同，表示预测值是准确的\n",
    "\n",
    "# 训练模型\n",
    "parameters = train(X_train, y_train, epochs=20, batch_size=128, lr=0.2)\n",
    "\n",
    "# 测试集预测\n",
    "test_cache = forward(X_test, parameters)\n",
    "test_acc = accuracy(y_test, test_cache[\"A2\"])\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bef1b-10a9-4e98-9d4d-70505c0c61cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
